{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ab353-efa5-4f50-91f8-3374dabe6a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åº”ç”¨ Dask é€šä¿¡é…ç½®...\n",
      "æ­£åœ¨å¯åŠ¨ Dask Client (Core: 7 Worker, 2 System)...\n",
      "Dask Dashboard: http://127.0.0.1:8787/status\n",
      "æ­£åœ¨åŠ è½½åŸºç¡€æ•°æ®...\n",
      "æ­£åœ¨æ„å»ºä»»åŠ¡é˜Ÿåˆ—...\n",
      "å¼€å§‹å¹¶è¡Œå¤„ç† 3009117 æ¡è·¯å¾„ (Batchæ¨¡å¼)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "å…¨å±€å¤„ç†è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [38:32<00:00, 210.24s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®¡ç®—å®Œæˆï¼Œå…±è·å–ç»“æœ 3009117 æ¡\n",
      "æ­£åœ¨ä¿å­˜ä¿®å¤åçš„ä¸­é—´æ–‡ä»¶...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 19:30:34,573 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.bag as db\n",
    "from tqdm import tqdm  # å¼•å…¥è¿›åº¦æ¡åº“\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ==========================================\n",
    "# æ ¸å¿ƒé…ç½®\n",
    "# ==========================================\n",
    "print(\"æ­£åœ¨åº”ç”¨ Dask é€šä¿¡é…ç½®...\")\n",
    "dask.config.set({\n",
    "    \"distributed.comm.timeouts.connect\": \"180s\",\n",
    "    \"distributed.comm.timeouts.tcp\": \"180s\",\n",
    "    \"distributed.comm.retry.count\": 5,\n",
    "    \"distributed.worker.memory.target\": 0.80,\n",
    "    \"distributed.worker.memory.spill\": 0.90,\n",
    "    \"distributed.worker.memory.pause\": 0.95\n",
    "})\n",
    "\n",
    "# --- æ–‡ä»¶è·¯å¾„é…ç½® ---\n",
    "DATA_DIR = '.' \n",
    "CENTER_ROADNET_PATH = os.path.join(DATA_DIR, 'center_roadnet')\n",
    "CENTER_POS_PATH = os.path.join(DATA_DIR, 'center_pos.npy')\n",
    "CENTER_REALPOS_PATH = os.path.join(DATA_DIR, 'center_realpos.npy')\n",
    "\n",
    "COMPLETE_PATH_COORDS_PATH = os.path.join(DATA_DIR, 'Complete_path_coords.pkl')\n",
    "COMPLETE_PATH_TIME_PATH = os.path.join(DATA_DIR, 'Complete_path_time.pkl')\n",
    "COMPLETE_PATH_DISTANCE_PATH = os.path.join(DATA_DIR, 'Complete_path_distance.pkl')\n",
    "COMPLETE_PATH_BAYONET_PATH = os.path.join(DATA_DIR, 'Complete_path_bayonet.pkl')\n",
    "\n",
    "TRAVEL_INFO_CSV_PATH = os.path.join(DATA_DIR, 'travel_info.csv')\n",
    "NODES_INFO_PKL_PATH = os.path.join(DATA_DIR, 'nodes_info.pkl')\n",
    "PATH_RESULT_CSV_PATH = os.path.join(DATA_DIR, 'path_result.csv')\n",
    "TIME_INTERVAL_DIST_XLSX_PATH = os.path.join(DATA_DIR, 'ç›¸é‚»è·¯ç½‘èŠ‚ç‚¹çš„æ—¶é—´é—´éš”åˆ†å¸ƒ.xlsx')\n",
    "VEHICLE_INFO_CSV_PATH = os.path.join(DATA_DIR, 'ä¸­å¿ƒåŸåŒºç ”ç©¶è½¦è¾†ä¿¡æ¯è¡¨.csv')\n",
    "\n",
    "FILE1_DIR = DATA_DIR \n",
    "FILE2_DIR = DATA_DIR \n",
    "\n",
    "COMPLETE_PATH_TF_PKL_PATH = os.path.join(FILE1_DIR, 'Complete_path_tf.pkl')\n",
    "COMPLETE_PATH_POS_PKL_PATH = os.path.join(FILE1_DIR, 'Complete_path_pos.pkl')\n",
    "PATH_PRED_TRAVEL_PKL_PATH = os.path.join(FILE1_DIR, 'path_pred_travel.pkl')\n",
    "PATH_POS_NUMBER_PKL_PATH = os.path.join(FILE1_DIR, 'path_pos_number.pkl')\n",
    "\n",
    "PATH_BAYONET_PKL_PATH = os.path.join(FILE2_DIR, 'final_path_bayonet.pkl')\n",
    "PATH_TIME_PKL_PATH = os.path.join(FILE2_DIR, 'final_path_time.pkl')\n",
    "PATH_START_TIME_PKL_PATH = os.path.join(FILE2_DIR, 'final_path_start_time.pkl')\n",
    "PATH_STOP_TIME_PKL_PATH = os.path.join(FILE2_DIR, 'final_path_stop_time.pkl')\n",
    "PATH_TRAVEL_PKL_PATH = os.path.join(FILE2_DIR, 'final_path_travel.pkl')\n",
    "PATH_CLTMBH_PKL_PATH = os.path.join(FILE2_DIR, 'final_path_cltmbh.pkl')\n",
    "PATH_BAYONET_NUMBER_NPY_PATH = os.path.join(FILE2_DIR, 'path_bayonet_number.npy')\n",
    "\n",
    "def load_raw_data():\n",
    "    print(\"æ­£åœ¨åŠ è½½åŸºç¡€æ•°æ®...\")\n",
    "    global G, pos_coord, pos_index_map, bayonet_name, realpos_index_map\n",
    "    G = nx.read_gml(CENTER_ROADNET_PATH)\n",
    "    pos = np.load(CENTER_POS_PATH, allow_pickle=True).item()\n",
    "    pos_name = list(pos.keys())\n",
    "    pos_coord = list(pos.values())\n",
    "    realpos = np.load(CENTER_REALPOS_PATH, allow_pickle=True)\n",
    "    realpos_name = list(realpos[0])\n",
    "    bayonet_name = list(realpos[1])\n",
    "    pos_index_map = {name: idx for idx, name in enumerate(pos_name)}\n",
    "    realpos_index_map = {name: idx for idx, name in enumerate(realpos_name)}\n",
    "\n",
    "    with open(COMPLETE_PATH_TF_PKL_PATH, 'rb') as f: Complete_path_tf = pickle.load(f)\n",
    "    with open(COMPLETE_PATH_POS_PKL_PATH, 'rb') as f: Complete_path_pos = pickle.load(f)\n",
    "    with open(PATH_PRED_TRAVEL_PKL_PATH, 'rb') as f: path_pred_travel = pickle.load(f)\n",
    "    with open(PATH_POS_NUMBER_PKL_PATH, 'rb') as f: path_pos_number = pickle.load(f)\n",
    "\n",
    "    path_bayonet = np.load(PATH_BAYONET_PKL_PATH, allow_pickle=True)\n",
    "    path_time = np.load(PATH_TIME_PKL_PATH, allow_pickle=True)\n",
    "    path_start_time = np.load(PATH_START_TIME_PKL_PATH, allow_pickle=True)\n",
    "    path_stop_time = np.load(PATH_STOP_TIME_PKL_PATH, allow_pickle=True)\n",
    "    path_travel = np.load(PATH_TRAVEL_PKL_PATH, allow_pickle=True)\n",
    "    path_cltmbh = np.load(PATH_CLTMBH_PKL_PATH, allow_pickle=True)\n",
    "    path_bayonet_number = np.load(PATH_BAYONET_NUMBER_NPY_PATH, allow_pickle=True)\n",
    "    return (Complete_path_tf, Complete_path_pos, path_travel, path_pred_travel, \n",
    "            path_pos_number, path_bayonet_number, path_cltmbh, \n",
    "            path_start_time, path_stop_time, path_time, path_bayonet)\n",
    "\n",
    "def calculate_single_trip_v2(args):\n",
    "    idx, status, path, travel_seconds, raw_start_time, raw_end_time = args\n",
    "    res = {'idx': idx, 'status': status, 'valid': False, 'nodes': None}\n",
    "    if status != \"get\" or not isinstance(path, list) or len(path) == 0: return res\n",
    "    try:\n",
    "        total_dist = 0\n",
    "        step_dists = [0]\n",
    "        if len(path) > 1:\n",
    "            for u, v in zip(path[:-1], path[1:]):\n",
    "                d = G.edges[u, v]['length']\n",
    "                total_dist += d\n",
    "                step_dists.append(round(d, 2))\n",
    "        total_dist = round(total_dist, 2)\n",
    "        velocity = round((total_dist / travel_seconds) * 3.6, 2) if travel_seconds > 0 else np.nan\n",
    "        coords = [pos_coord[pos_index_map[p]] for p in path]\n",
    "        bayonets = [bayonet_name[realpos_index_map[p]] if p in realpos_index_map else np.nan for p in path]\n",
    "        times = [raw_start_time]\n",
    "        current_time = raw_start_time\n",
    "        if total_dist > 0:\n",
    "            for d in step_dists[1:]:\n",
    "                rate = d / total_dist\n",
    "                add_sec = max(0, round(travel_seconds * rate))\n",
    "                current_time += datetime.timedelta(seconds=add_sec)\n",
    "                times.append(current_time)\n",
    "        else:\n",
    "            times = [raw_start_time] * len(path)\n",
    "        times[-1] = raw_end_time\n",
    "        res.update({'total_distance': total_dist, 'velocity': velocity, 'step_distances': step_dists,\n",
    "                    'step_coords': coords, 'step_bayonets': bayonets, 'step_times': times,\n",
    "                    'nodes': path, 'valid': True})\n",
    "        return res\n",
    "    except Exception: return res\n",
    "\n",
    "def flatten_trip_to_rows(trip_data, cltmbh):\n",
    "    if not trip_data['valid']: return []\n",
    "    rows = []\n",
    "    c_distance = 0\n",
    "    travel_id = trip_data['idx']\n",
    "    dists, times, bays, coords, nodes = trip_data['step_distances'], trip_data['step_times'], trip_data['step_bayonets'], trip_data['step_coords'], trip_data['nodes']\n",
    "    for i in range(len(nodes)):\n",
    "        dist = dists[i]\n",
    "        c_distance += dist\n",
    "        rows.append({'device_id': cltmbh, 'travel_id': travel_id, 'datetime': times[i],\n",
    "                     'bayonetname': bays[i], 'posname': nodes[i], 'longitude': coords[i][0],\n",
    "                     'latitude': coords[i][1], 'c_distance': c_distance, 'distance': dist})\n",
    "    return rows\n",
    "\n",
    "def main():\n",
    "    # ä¸ºäº†ä¿è¯è¿›åº¦æ¡æ˜¾ç¤ºæ­£ç¡®ï¼Œä¸æ˜¾ç¤º Dask å†…éƒ¨çš„è¿›åº¦æ¡\n",
    "    # from dask.diagnostics import ProgressBar  <-- ç§»é™¤è¿™ä¸ªï¼Œç”¨ tqdm ä»£æ›¿\n",
    "\n",
    "    print(f\"æ­£åœ¨å¯åŠ¨ Dask Client (Core: 7 Worker, 2 System)...\")\n",
    "    cluster = LocalCluster(n_workers=7, threads_per_worker=1, memory_limit='24GB')\n",
    "    client = Client(cluster)\n",
    "    print(f\"Dask Dashboard: {client.dashboard_link}\")\n",
    "\n",
    "    (Complete_path_tf, Complete_path_pos, path_travel, path_pred_travel, path_pos_number, \n",
    "     path_bayonet_number, path_cltmbh, path_start_time, path_stop_time, path_time, path_bayonet) = load_raw_data()\n",
    "\n",
    "    print(\"æ­£åœ¨æ„å»ºä»»åŠ¡é˜Ÿåˆ—...\")\n",
    "    tasks = []\n",
    "    total = len(Complete_path_tf)\n",
    "    for i in range(total):\n",
    "        if isinstance(path_time[i], list) and len(path_time[i]) > 0: s_time, e_time = path_time[i][0], path_time[i][-1]\n",
    "        else: s_time, e_time = path_start_time[i], path_stop_time[i]\n",
    "        tasks.append((i, Complete_path_tf[i], Complete_path_pos[i], path_travel[i], s_time, e_time))\n",
    "\n",
    "    print(f\"å¼€å§‹å¹¶è¡Œå¤„ç† {total} æ¡è·¯å¾„ (Batchæ¨¡å¼)...\")\n",
    "    \n",
    "    # 1. ä»»åŠ¡å…¥è¢‹ & æŒ‚è½½å‡½æ•°\n",
    "    bag = db.from_sequence(tasks, npartitions=100).map(calculate_single_trip_v2)\n",
    "    \n",
    "    # 2. è½¬æ¢ä¸º Delayed åˆ—è¡¨\n",
    "    delayed_list = bag.to_delayed()\n",
    "    \n",
    "    results = []\n",
    "    batch_size = 10 \n",
    "    \n",
    "    # ==========================================\n",
    "    # ã€ä¿®æ”¹ç‚¹ã€‘å¼•å…¥ tqdm å…¨å±€è¿›åº¦æ¡\n",
    "    # ==========================================\n",
    "    # tqdm ä¼šè‡ªåŠ¨è®¡ç®—æ€»æ‰¹æ¬¡æ•°ï¼Œå¹¶æ˜¾ç¤ºå‰©ä½™æ—¶é—´\n",
    "    # range(0, len(delayed_list), batch_size) å†³å®šäº†å¾ªç¯æ¬¡æ•°\n",
    "    \n",
    "    for i in tqdm(range(0, len(delayed_list), batch_size), desc=\"å…¨å±€å¤„ç†è¿›åº¦\", unit=\"batch\"):\n",
    "        batch = delayed_list[i : i + batch_size]\n",
    "        \n",
    "        # æ‰§è¡Œè®¡ç®—\n",
    "        batch_results = dask.compute(*batch)\n",
    "        \n",
    "        for part in batch_results: \n",
    "            results.extend(part)\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"è®¡ç®—å®Œæˆï¼Œå…±è·å–ç»“æœ {len(results)} æ¡\")\n",
    "    \n",
    "    # ç»“æœæ’åº\n",
    "    results.sort(key=lambda x: x['idx'])\n",
    "    \n",
    "    rec_dist, rec_coords, rec_bayonet, rec_time = [], [], [], []\n",
    "    travel_info_rows, valid_results = [], []\n",
    "    \n",
    "    for i, res in enumerate(results):\n",
    "        if res['valid']:\n",
    "            rec_dist.append(res['step_distances'])\n",
    "            rec_coords.append(res['step_coords'])\n",
    "            rec_bayonet.append(res['step_bayonets'])\n",
    "            rec_time.append(res['step_times'])\n",
    "            travel_info_rows.append({'CLTMBH': path_cltmbh[i], 'start_time': path_start_time[i],\n",
    "                'stop_time': path_stop_time[i], 'travel_time_s': path_travel[i], 'predtravel_time_s': path_pred_travel[i],\n",
    "                'distance_km': round(res['total_distance'] / 1000, 2), 'speed_kmh': res['velocity'],\n",
    "                'bayonet_records': path_bayonet_number[i], 'node_records': path_pos_number[i],\n",
    "                'start_bayonet': res['step_bayonets'][0], 'stop_bayonet': res['step_bayonets'][-1],\n",
    "                'start_node_lon': res['step_coords'][0][0], 'start_node_lat': res['step_coords'][0][1],\n",
    "                'stop_node_lon': res['step_coords'][-1][0], 'stop_node_lat': res['step_coords'][-1][1], 'position': i})\n",
    "            valid_results.append(res)\n",
    "        else:\n",
    "            rec_dist.append(np.nan); rec_coords.append(np.nan); rec_bayonet.append(np.nan); rec_time.append(np.nan)\n",
    "\n",
    "    print(\"æ­£åœ¨ä¿å­˜ä¿®å¤åçš„ä¸­é—´æ–‡ä»¶...\")\n",
    "    joblib.dump(rec_dist, COMPLETE_PATH_DISTANCE_PATH)\n",
    "    joblib.dump(rec_coords, COMPLETE_PATH_COORDS_PATH)\n",
    "    joblib.dump(rec_bayonet, COMPLETE_PATH_BAYONET_PATH)\n",
    "    joblib.dump(rec_time, COMPLETE_PATH_TIME_PATH)\n",
    "\n",
    "    print(\"æ­£åœ¨ç”Ÿæˆ travel_info.csv ...\")\n",
    "    travel_df = pd.DataFrame(travel_info_rows)\n",
    "    if os.path.exists(VEHICLE_INFO_CSV_PATH):\n",
    "        veh_info = pd.read_csv(VEHICLE_INFO_CSV_PATH, encoding='gbk')\n",
    "        travel_df = pd.merge(travel_df, veh_info[['CLTMBH', 'veh_type', 'fuel_type', 'PFBZ']], on='CLTMBH', how='left')\n",
    "    \n",
    "    travel_df = travel_df[(travel_df['speed_kmh'] <= 60) & (travel_df['distance_km'] >= 0.5)].copy()\n",
    "    def check_error(row):\n",
    "        a, b = row['predtravel_time_s'], row['travel_time_s']\n",
    "        if pd.isna(a) or pd.isna(b) or b <= 0: return False\n",
    "        return (b * 0.4) < a < (b * 1.5)\n",
    "    travel_df = travel_df[travel_df.apply(check_error, axis=1)]\n",
    "    travel_df.to_csv(TRAVEL_INFO_CSV_PATH, encoding='gbk', index=False)\n",
    "    \n",
    "    valid_positions = travel_df['position'].tolist()\n",
    "    nodes_info = [[Complete_path_pos[i] for i in valid_positions], [rec_coords[i] for i in valid_positions],\n",
    "                  [rec_time[i] for i in valid_positions], [rec_dist[i] for i in valid_positions], [rec_bayonet[i] for i in valid_positions]]\n",
    "    with open(NODES_INFO_PKL_PATH, 'wb') as f: pickle.dump(nodes_info, f)\n",
    "\n",
    "    print(\"æ­£åœ¨ç”Ÿæˆ path_result.csv ...\")\n",
    "    valid_pos_set = set(valid_positions)\n",
    "    final_explode_tasks = [(res, path_cltmbh[res['idx']]) for res in valid_results if res['idx'] in valid_pos_set]\n",
    "    bag_explode = db.from_sequence(final_explode_tasks, npartitions=54)\n",
    "    ddf = bag_explode.map(lambda args: flatten_trip_to_rows(*args)).flatten().to_dataframe()\n",
    "    ddf.to_csv(PATH_RESULT_CSV_PATH, index=False, single_file=True, encoding='gbk')\n",
    "\n",
    "    print(\"æ­£åœ¨ç”Ÿæˆæ—¶é—´é—´éš”ç»Ÿè®¡...\")\n",
    "    all_intervals = []\n",
    "    for times in rec_time:\n",
    "        if isinstance(times, list) and len(times) > 1: all_intervals.extend([(times[j]-times[j-1]).seconds for j in range(1, len(times))])\n",
    "    if all_intervals:\n",
    "        interval_bins = [0, 5, 15, 30, 60, 300, 900, 1800, 3600, 28800, 43200, 86400]\n",
    "        res_cut = pd.cut(all_intervals, bins=interval_bins, labels=interval_bins[:-1], right=False)\n",
    "        pd.value_counts(res_cut, sort=False).reset_index().to_excel(TIME_INTERVAL_DIST_XLSX_PATH, index=False)\n",
    "\n",
    "    print(\"ğŸ‰ æ‰€æœ‰ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼\")\n",
    "    client.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2644b-20e0-462d-8746-336af1824c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"bokeh>=3.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208199c-9d99-4f7b-a11a-313161759455",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7149a1-b372-4926-b757-711ff5559ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
