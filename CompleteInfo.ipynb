{
 "cells": [
  {
   "cell_type": "code",
   "id": "8ed392f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T07:20:50.187140Z",
     "start_time": "2025-08-07T07:20:49.731607Z"
    }
   },
   "source": [
    "# 补全路径规划信息 20250128 by chenxinyi\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9a1f29b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T07:20:50.757536Z",
     "start_time": "2025-08-07T07:20:50.753792Z"
    }
   },
   "source": [
    "def get_path_weight(G, path, weight):\n",
    "    value = 0\n",
    "    if len(path) > 1:\n",
    "        for i in range(len(path)-1):\n",
    "            u = path[i]\n",
    "            v = path[i+1]\n",
    "            value += G.edges[u,v][weight]\n",
    "    return value\n",
    "\n",
    "\n",
    "def get_path_stepcoords(path, posname, poscoord):\n",
    "    points = []\n",
    "    for p in path:\n",
    "        index = posname.index(p)\n",
    "        coord = poscoord[index]\n",
    "        points.append(coord)\n",
    "    return points\n",
    "\n",
    "\n",
    "def get_path_stepbayonet(path, posname, bayonetname):\n",
    "    bayonet_array = []\n",
    "    for i in range(0, len(path)):\n",
    "        try:\n",
    "            index = posname.index(path[i])\n",
    "            bayonet = bayonetname[index]\n",
    "            bayonet_array.append(bayonet)\n",
    "        except:\n",
    "            bayonet_array.append(np.nan)\n",
    "            continue\n",
    "    return bayonet_array\n",
    "\n",
    "\n",
    "def get_path_steplength(G, path):\n",
    "    array = [0]\n",
    "    if len(path) > 1:\n",
    "        for i in range(len(path)-1):\n",
    "            u = path[i]\n",
    "            v = path[i+1]\n",
    "            length = round(G.edges[u,v]['length'], 2)\n",
    "            array.append(length)\n",
    "    return array"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e36709d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T07:20:53.638967Z",
     "start_time": "2025-08-07T07:20:52.189627Z"
    }
   },
   "source": [
    "# 读取路网拓扑结构\n",
    "G = nx.read_gml(r'C:\\01 毕业论文\\7.论文代码和结果\\硕士毕业论文代码\\车辆轨迹提取\\center_roadnet')\n",
    "pos = np.load(r'C:\\01 毕业论文\\7.论文代码和结果\\硕士毕业论文代码\\车辆轨迹提取\\center_pos.npy', allow_pickle=True).item()\n",
    "pos_name = list(pos.keys())\n",
    "pos_coord = list(pos.values())\n",
    "realpos = np.load(r'C:\\01 毕业论文\\7.论文代码和结果\\硕士毕业论文代码\\车辆轨迹提取\\center_realpos.npy')\n",
    "realpos_name = list(realpos[0])\n",
    "bayonet_name = list(realpos[1])"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a3101639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T07:21:32.145503Z",
     "start_time": "2025-08-07T07:21:00.904409Z"
    }
   },
   "source": [
    "# # 读取重构路径信息\n",
    "# \n",
    "# file1 = '路径重构结果/全部处理结果'  # 路径规划后的结果\n",
    "# Complete_path_tf = list(np.load(file1 + '/' + 'Complete_path_tf.npy', allow_pickle=True))\n",
    "# Complete_path_pos = list(np.load(file1 + '/' + 'Complete_path_pos.npy', allow_pickle=True))\n",
    "# path_pred_travel = list(np.load(file1 + '/' + 'path_pred_travel.npy'))\n",
    "# path_pos_number = list(np.load(file1 + '/' + 'path_pos_number.npy'))\n",
    "# \n",
    "# # 筛选路径规划成功的出行路径所在列表位置\n",
    "# positions = [index for index, value in enumerate(Complete_path_tf) if value == 'get']\n",
    "# \n",
    "# file2 = '../车辆轨迹划分/中心城区单次出行轨迹结果'  # 轨迹划分后的结果\n",
    "# # 单次出行轨迹经过的对应卡口设备\n",
    "# path_bayonet_file = np.load(file2 + '/' + 'path_bayonet.npy', allow_pickle=True)\n",
    "# path_bayonet = path_bayonet_file.tolist()\n",
    "# # 经过每一卡口设备的时间\n",
    "# path_time_file = np.load(file2 + '/' + 'path_time.npy', allow_pickle=True)\n",
    "# path_time = path_time_file.tolist()\n",
    "# # 起始时间\n",
    "# path_start_time = list(np.load(file2 + '/' + 'path_start_time.npy', allow_pickle=True))\n",
    "# # 结束时间\n",
    "# path_stop_time = list(np.load(file2 + '/' + 'path_stop_time.npy', allow_pickle=True))\n",
    "# # 行程时间\n",
    "# path_travel = list(np.load(file2 + '/' + 'path_travel.npy'))\n",
    "# # 车辆号牌\n",
    "# path_cltmbh = list(np.load(file2 + '/' + 'path_cltmbh.npy'))\n",
    "# # 原始数据经过的卡口个数\n",
    "# path_bayonet_number = list(np.load(file2 + '/' + 'path_bayonet_number.npy'))\n",
    "# \n",
    "# print('单次出行路径:', len(Complete_path_tf))\n",
    "# print('路径规划后的单次出行路径:', len(positions), '占比: {:.2%}'.format(len(positions) / len(Complete_path_tf)))\n",
    "# 读取重构路径信息\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "file1 = r'C:\\01 毕业论文\\7.论文代码和结果\\硕士毕业论文代码\\车辆轨迹提取'  # 路径规划后的结果\n",
    "\n",
    "# 使用pickle加载路径规划结果\n",
    "with open(file1 + '/Complete_path_tf.pkl', 'rb') as f:\n",
    "    Complete_path_tf = pickle.load(f)\n",
    "with open(file1 + '/Complete_path_pos.pkl', 'rb') as f:\n",
    "    Complete_path_pos = pickle.load(f)\n",
    "with open(file1 + '/path_pred_travel.pkl', 'rb') as f:\n",
    "    path_pred_travel = pickle.load(f)\n",
    "with open(file1 + '/path_pos_number.pkl', 'rb') as f:\n",
    "    path_pos_number = pickle.load(f)\n",
    "\n",
    "# 筛选路径规划成功的出行路径所在列表位置\n",
    "positions = [index for index, value in enumerate(Complete_path_tf) if value == 'get']\n",
    "\n",
    "file2 = r'C:\\01 毕业论文\\7.论文代码和结果\\硕士毕业论文代码\\车辆轨迹提取'  # 轨迹划分后的结果\n",
    "\n",
    "# 使用numpy加载轨迹划分结果\n",
    "# 单次出行轨迹经过的对应卡口设备\n",
    "path_bayonet = np.load(file2 + '/final_path_bayonet.pkl', allow_pickle=True)\n",
    "# 经过每一卡口设备的时间\n",
    "path_time = np.load(file2 + '/final_path_time.pkl', allow_pickle=True)\n",
    "# 起始时间\n",
    "path_start_time = np.load(file2 + '/final_path_start_time.pkl', allow_pickle=True)\n",
    "# 结束时间\n",
    "path_stop_time = np.load(file2 + '/final_path_stop_time.pkl', allow_pickle=True)\n",
    "# 行程时间\n",
    "path_travel = np.load(file2 + '/final_path_travel.pkl', allow_pickle=True)\n",
    "# 车辆号牌\n",
    "path_cltmbh = np.load(file2 + '/final_path_cltmbh.pkl', allow_pickle=True)\n",
    "# 原始数据经过的卡口个数\n",
    "path_bayonet_number = np.load(file2 + '/path_bayonet_number.npy', allow_pickle=True)\n",
    "\n",
    "# 确保所有加载的数据转换为列表类型\n",
    "if isinstance(path_bayonet, np.ndarray):\n",
    "    path_bayonet = path_bayonet.tolist()\n",
    "if isinstance(path_time, np.ndarray):\n",
    "    path_time = path_time.tolist()\n",
    "if isinstance(path_start_time, np.ndarray):\n",
    "    path_start_time = path_start_time.tolist()\n",
    "if isinstance(path_stop_time, np.ndarray):\n",
    "    path_stop_time = path_stop_time.tolist()\n",
    "if isinstance(path_travel, np.ndarray):\n",
    "    path_travel = path_travel.tolist()\n",
    "if isinstance(path_cltmbh, np.ndarray):\n",
    "    path_cltmbh = path_cltmbh.tolist()\n",
    "if isinstance(path_bayonet_number, np.ndarray):\n",
    "    path_bayonet_number = path_bayonet_number.tolist()\n",
    "\n",
    "print('单次出行路径总数:', len(Complete_path_tf))\n",
    "print('路径规划成功的出行路径数:', len(positions), \n",
    "      '占比: {:.2%}'.format(len(positions) / len(Complete_path_tf)))"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6994a4e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T07:22:29.235549Z",
     "start_time": "2025-08-07T07:21:38.074980Z"
    }
   },
   "source": [
    "# 补全单次出行的总行驶距离（m）、平均速度（km/h）\n",
    "\n",
    "path_distance = []    # 存储每一次单次出行的总行驶距离 单位:m\n",
    "path_velocity = []    # 存储每一次单次出行的平均行驶速度 单位:km/h\n",
    "\n",
    "for i in range(0, len(Complete_path_tf)):\n",
    "    if Complete_path_tf[i] == \"get\":\n",
    "        try:\n",
    "            total_distance = get_path_weight(G, Complete_path_pos[i], 'length')\n",
    "            total_distance = round(total_distance, 2)\n",
    "            velocity = (total_distance / path_travel[i]) * 3.6\n",
    "            velocity = round(velocity, 2)\n",
    "            path_distance.append(total_distance)\n",
    "            path_velocity.append(velocity)\n",
    "        except:\n",
    "            print(i, ': Function(get_path_weight) fail')\n",
    "            Complete_path_tf[i] = -1\n",
    "            path_distance.append(np.nan)\n",
    "            path_velocity.append(np.nan)\n",
    "            continue\n",
    "    else:\n",
    "        path_distance.append(np.nan)\n",
    "        path_velocity.append(np.nan)\n",
    "\n",
    "print(f'* path_distance records: {len(path_distance)}')\n",
    "print(path_distance[195000:195010])\n",
    "print(f'* path_velocity records: {len(path_velocity)}')\n",
    "print(path_velocity[195000:195010])"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "db67df8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T15:42:05.546786Z",
     "start_time": "2025-08-07T07:27:16.686057Z"
    }
   },
   "source": [
    "# 补全每一路网节点对应的过车时刻、经纬度、行驶距离、卡口编号（没有则为空）\n",
    "\n",
    "Complete_path_bayonet = []     # 存储每个路网节点对应的卡口编号\n",
    "Complete_path_coords = []      # 存储每个路网节点的经纬度坐标\n",
    "Complete_path_distance = []    # 存储从上一个路网节点到当前节点的行驶距离（m）\n",
    "Complete_path_time = []        # 存储经过每个路网节点的时刻\n",
    "\n",
    "for i in range(0, len(Complete_path_tf)):\n",
    "    \n",
    "    if Complete_path_tf[i] == \"get\":\n",
    "        \n",
    "        # todo 计算距离、经纬度、对应卡口设备\n",
    "        distances = get_path_steplength(G, Complete_path_pos[i])\n",
    "        coords = get_path_stepcoords(Complete_path_pos[i], pos_name, pos_coord)\n",
    "        bayonets = get_path_stepbayonet(Complete_path_pos[i], realpos_name, bayonet_name)\n",
    "        \n",
    "        Complete_path_distance.append(distances)\n",
    "        Complete_path_coords.append(coords)\n",
    "        Complete_path_bayonet.append(bayonets)\n",
    "        \n",
    "        # todo 计算过车时刻和瞬时速度\n",
    "        start_time = path_time[i][0]\n",
    "        end_time = path_time[i][-1]\n",
    "        times = [start_time]\n",
    "        \n",
    "        # 遍历每一路网节点\n",
    "        single_path_pos = Complete_path_pos[i]\n",
    "        total_dist = path_distance[i]\n",
    "        total_travel = path_travel[i]\n",
    "        current_time = start_time\n",
    "        for j in range(1, len(single_path_pos)):\n",
    "            # 计算当前节点的行驶距离与单次出行总距离之比\n",
    "            rate = Complete_path_distance[i][j] / total_dist\n",
    "            # 根据距离比，等比计算时间差(s)\n",
    "            trange = round(total_travel * rate)\n",
    "            # 计算当前节点的过车时间\n",
    "            current_time = current_time + datetime.timedelta(seconds=trange)\n",
    "            times.append(current_time)\n",
    "        \n",
    "        times[-1] = end_time\n",
    "        Complete_path_time.append(times)\n",
    "        \n",
    "        # 每次出行的数据维度校验\n",
    "        # print(f\"{i}  {len(Complete_path_distance[i])} {len(Complete_path_coords[i])} {len(Complete_path_bayonet[i])} {len(Complete_path_time[i])}\" )\n",
    "        # print(\"Complete_path_distance:\", Complete_path_distance[i])\n",
    "        # print(\"Complete_paths_time:\", Complete_path_time[i])\n",
    "        \n",
    "    else:\n",
    "        Complete_path_distance.append(np.nan)\n",
    "        Complete_path_coords.append(np.nan)\n",
    "        Complete_path_bayonet.append(np.nan)\n",
    "        Complete_path_time.append(np.nan)\n",
    "    \n",
    "    # 定期打印算法运行时间和保存结果\n",
    "    if i in list(np.arange(20000, 3009117, 20000)):\n",
    "        print(f\"第{i}次出行 信息补全已完成\")\n",
    "print(1)"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "551c5d64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T07:40:48.074132Z",
     "start_time": "2025-08-08T07:40:39.682870Z"
    }
   },
   "source": [
    "# 打印特定索引的数据\n",
    "print(f'* Complete_path_distance records: {len(Complete_path_distance)}')\n",
    "print(Complete_path_distance[195003])\n",
    "print(f'* Complete_path_coords records: {len(Complete_path_coords)}')\n",
    "print(Complete_path_coords[195003])\n",
    "print(f'* Complete_path_bayonet records: {len(Complete_path_bayonet)}')\n",
    "print(Complete_path_bayonet[195003])\n",
    "print(f'* Complete_path_time records: {len(Complete_path_time)}')\n",
    "print(Complete_path_time[195003])\n",
    "import pickle # 确保在文件顶部或此代码块附近导入了 pickle\n",
    "\n",
    "# 保存全部处理结果 (使用 pickle 格式)\n",
    "with open(file1 + '/' + 'Complete_path_distance.pkl', 'wb') as f:\n",
    "    pickle.dump(Complete_path_distance, f)\n",
    "\n",
    "# # 定义批量大小\n",
    "# batch_size = 10000\n",
    "# \n",
    "# # 分批保存数据\n",
    "# def save_in_batches(data, filename, batch_size):\n",
    "#     num_batches = (len(data) + batch_size - 1) // batch_size\n",
    "#     for i in range(num_batches):\n",
    "#         start_idx = i * batch_size\n",
    "#         end_idx = min((i + 1) * batch_size, len(data))\n",
    "#         batch_filename = f\"{filename}_batch_{i}.pkl\"\n",
    "#         with open(batch_filename, 'wb') as f:\n",
    "#             pickle.dump(data[start_idx:end_idx], f)\n",
    "#         print(f'Saved batch {i} to {batch_filename}')\n",
    "# \n",
    "# save_in_batches(Complete_path_distance, file1 + '/' + 'Complete_path_distance', batch_size)\n",
    "# save_in_batches(Complete_path_coords, file1 + '/' + 'Complete_path_coords', batch_size)\n",
    "# save_in_batches(Complete_path_bayonet, file1 + '/' + 'Complete_path_bayonet', batch_size)\n",
    "# save_in_batches(Complete_path_time, file1 + '/' + 'Complete_path_time', batch_size)\n"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T07:42:07.251137Z",
     "start_time": "2025-08-08T07:41:51.761295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(file1 + '/' + 'Complete_path_coords.pkl', 'wb') as f:\n",
    "    pickle.dump(Complete_path_coords, f)"
   ],
   "id": "37b874f506b88ffa",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T07:42:23.570463Z",
     "start_time": "2025-08-08T07:42:15.233855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(file1 + '/' + 'Complete_path_bayonet.pkl', 'wb') as f:\n",
    "    pickle.dump(Complete_path_bayonet, f)"
   ],
   "id": "9d99f0bbdbd65b91",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T07:59:51.199969Z",
     "start_time": "2025-08-08T07:59:51.187930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.getsizeof(Complete_path_time) / (1024**2)  # 单位 MB\n"
   ],
   "id": "12ebb8c849929942",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "ec8ab8b413def35e",
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-08T08:12:13.024020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# with open(file1 + '/' + 'Complete_path_time.pkl', 'wb') as f:\n",
    "#     pickle.dump(Complete_path_time, f)\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "save_path = os.path.join(file1, 'Complete_path_time.pkl')\n",
    "joblib.dump(Complete_path_time, save_path, compress=3)\n"
   ],
   "id": "4a1423aff008dad3",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "0217d0e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:45:32.369998Z",
     "start_time": "2025-08-04T21:45:32.350730Z"
    }
   },
   "source": [
    "print('数据维度检验:')\n",
    "print(len(path_cltmbh), len(path_travel), len(path_pred_travel), len(path_start_time), len(path_stop_time), \\\n",
    "      len(path_distance), len(path_velocity), len(path_pos_number), \\\n",
    "      len(Complete_path_tf), len(Complete_path_pos), len(Complete_path_coords), len(Complete_path_bayonet), \\\n",
    "      len(Complete_path_time), len(Complete_path_distance))"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "75013ff4",
   "metadata": {},
   "source": [
    "# 剔除异常出行数据"
   ]
  },
  {
   "cell_type": "code",
   "id": "27a4dc22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:45:33.400628Z",
     "start_time": "2025-08-04T21:45:32.373Z"
    }
   },
   "source": [
    "# # 读取信息补全后的结果\n",
    "# # file1 = '路径重构结果/全部处理结果'\n",
    "# # Complete_path_tf = list(np.load(file1 + '/' + 'Complete_path_tf.npy', allow_pickle=True))\n",
    "# # Complete_path_pos = list(np.load(file1 + '/' + 'Complete_path_pos.npy', allow_pickle=True))\n",
    "# Complete_path_coords = list(np.load(file1 + '/' + 'Complete_path_coords.npy', allow_pickle=True))\n",
    "# Complete_path_time = list(np.load(file1 + '/' + 'Complete_path_time.npy', allow_pickle=True))\n",
    "# Complete_path_distance = list(np.load(file1 + '/' + 'Complete_path_distance.npy', allow_pickle=True))\n",
    "# Complete_path_bayonet = list(np.load(file1 + '/' + 'Complete_path_bayonet.npy', allow_pickle=True))\n",
    "# \n",
    "# # 保存路径规划结果（仅包括路径规划成功的数据）\n",
    "# save_file = '路径重构结果/有效处理结果'\n",
    "# \n",
    "# # 车辆号牌\n",
    "# path_cltmbh = list(np.load('../车辆轨迹划分/中心城区单次出行轨迹结果/path_cltmbh.npy'))\n",
    "# \n",
    "# # 筛选路径规划成功的出行路径所在列表位置\n",
    "# positions = [index for index, value in enumerate(Complete_path_tf) if value == 'get']\n",
    "# print('补全信息后的单次出行路径:', len(positions), '占比: {:.2%}'.format(len(positions) / len(Complete_path_tf)))\n",
    "# 读取信息补全后的结果\n",
    "\n",
    "\n",
    "# # 使用pickle加载路径规划结果\n",
    "# with open(file1 + '/Complete_path_tf.pkl', 'rb') as f:\n",
    "#     Complete_path_tf = pickle.load(f)\n",
    "# with open(file1 + '/Complete_path_pos.pkl', 'rb') as f:\n",
    "#     Complete_path_pos = pickle.load(f)\n",
    "#     \n",
    "# 加载其他补全信息（假设这些文件也是以pickle格式保存）\n",
    "# with open(file1 + '/Complete_path_coords.pkl', 'rb') as f:\n",
    "#     Complete_path_coords = pickle.load(f)\n",
    "# with open(file1 + '/Complete_path_time.pkl', 'rb') as f:\n",
    "#     Complete_path_time = pickle.load(f)\n",
    "# with open(file1 + '/Complete_path_distance.pkl', 'rb') as f:\n",
    "#     Complete_path_distance = pickle.load(f)\n",
    "# with open(file1 + '/Complete_path_bayonet.pkl', 'rb') as f:\n",
    "#     Complete_path_bayonet = pickle.load(f)\n",
    "\n",
    "# 保存路径规划结果（仅包括路径规划成功的数据）\n",
    "save_file = file1\n",
    "\n",
    "# 加载车辆号牌数据（根据之前代码的保存格式）\n",
    "with open(file1 + '/final_path_cltmbh.pkl', 'rb') as f:\n",
    "    path_cltmbh = pickle.load(f)\n",
    "\n",
    "# 筛选路径规划成功的出行路径所在列表位置\n",
    "positions = [index for index, value in enumerate(Complete_path_tf) if value == 'get']\n",
    "print('补全信息后的单次出行路径:', len(positions), '占比: {:.2%}'.format(len(positions) / len(Complete_path_tf)))"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:45:33.499298Z",
     "start_time": "2025-08-04T21:45:33.415595Z"
    }
   },
   "cell_type": "code",
   "source": "path_bayonet_number = np.load(file2 + '/path_bayonet_number.npy', allow_pickle=True)",
   "id": "ff2731dfdfdb54fb",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:45:33.534706Z",
     "start_time": "2025-08-04T21:45:33.500479Z"
    }
   },
   "cell_type": "code",
   "source": "len(path_bayonet_number)",
   "id": "928e2029b4869314",
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "23a35ade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:46:19.782995Z",
     "start_time": "2025-08-04T21:45:33.536846Z"
    }
   },
   "source": [
    "# 创建单次出行信息表\n",
    "\n",
    "travel_info = {'CLTMBH': [path_cltmbh[i] for i in positions],  \n",
    "               'start_time': [path_start_time[i] for i in positions],\n",
    "               'stop_time': [path_stop_time[i] for i in positions], \n",
    "               'travel_time_s': [path_travel[i] for i in positions], \n",
    "               'predtravel_time_s': [path_pred_travel[i] for i in positions],\n",
    "               'distance_km': [round(path_distance[i]/1000, 2) for i in positions],\n",
    "               'speed_kmh': [path_velocity[i] for i in positions],\n",
    "               'bayonet_records': [path_bayonet_number[i] for i in positions],\n",
    "               'node_records': [path_pos_number[i] for i in positions], \n",
    "               'start_bayonet': [Complete_path_bayonet[i][0] for i in positions], \n",
    "               'stop_bayonet': [Complete_path_bayonet[i][-1] for i in positions], \n",
    "               'start_node_lon': [Complete_path_coords[i][0][0] for i in positions], \n",
    "               'start_node_lat': [Complete_path_coords[i][0][1] for i in positions],\n",
    "               'stop_node_lon': [Complete_path_coords[i][-1][0] for i in positions], \n",
    "               'stop_node_lat': [Complete_path_coords[i][-1][1] for i in positions],\n",
    "               'position': positions\n",
    "               }\n",
    "travel_info = pd.DataFrame(travel_info)\n",
    "\n",
    "# # 匹配车辆信息表\n",
    "vehicle_info = pd.read_csv(r'D:\\24216373郑金涛中微观研究\\中心城区研究车辆信息表.csv', encoding='gbk')\n",
    "travel_info = pd.merge(travel_info, vehicle_info[['CLTMBH','veh_type','fuel_type','PFBZ']], on='CLTMBH', how='left')\n",
    "print('travel_info:', len(travel_info))"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "dd43030e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:46:20.284318Z",
     "start_time": "2025-08-04T21:46:19.783969Z"
    }
   },
   "source": [
    "# todo 剔除平均速度＞60km/h 或 距离＜0.5km的路径\n",
    "speed_threshold = 60\n",
    "distance_threshold = 0.5\n",
    "\n",
    "# 提取有效出行的索引\n",
    "valid_positions = travel_info.loc[(travel_info['speed_kmh'] <= speed_threshold) & \\\n",
    "                                  (travel_info['distance_km'] >= distance_threshold)]['position'].tolist()\n",
    "\n",
    "# 统计异常出行\n",
    "abnormal_records = len(travel_info) - len(valid_positions)\n",
    "print(f\"平均速度＞{speed_threshold}km/h 或 距离＜{distance_threshold}km的异常出行: {abnormal_records}\",\n",
    "      \"占比: {:.2%}\".format(abnormal_records / len(travel_info)))\n",
    "\n",
    "# 提取有效出行数据\n",
    "valid_travel_info = travel_info[travel_info['position'].isin(valid_positions)]\n",
    "valid_travel_info.reset_index(drop=True, inplace=True)\n",
    "print(f\"剔除速度和距离异常后的出行次数: {len(valid_travel_info)} / {len(travel_info)}\", \n",
    "      \"占比: {:.2%}\".format((len(valid_travel_info)) / len(travel_info)))"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "57add90e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:46:31.832103Z",
     "start_time": "2025-08-04T21:46:20.285504Z"
    }
   },
   "source": [
    "# todo 剔除估计行程时间与实际行程时间相差较大的路径\n",
    "\n",
    "# 提取有效出行的索引\n",
    "def function(a, b):\n",
    "    minscaler = round(b*0.4)\n",
    "    maxscaler = round(b*1.5)\n",
    "    if a > minscaler and a < maxscaler:\n",
    "        return '总时间误差满足'\n",
    "    else:\n",
    "        return '总时间误差不满足'\n",
    "\n",
    "travel_info['travel_error'] = travel_info.apply(lambda x : function(x['predtravel_time_s'],x['travel_time_s']), axis = 1)\n",
    "valid_positions2 = travel_info.loc[(travel_info['speed_kmh'] <= speed_threshold) & \\\n",
    "                                   (travel_info['distance_km'] >= distance_threshold) & \\\n",
    "                                   (travel_info['travel_error'] == '总时间误差满足')]['position'].tolist()\n",
    "\n",
    "abnormal_records2 = len(travel_info) - len(valid_positions2)\n",
    "print(f\"预估行程时间超过 实际行程时间*(0.4~1.5) 区间内的异常出行: {abnormal_records2}\",\n",
    "      \"占比: {:.2%}\".format(abnormal_records2 / len(travel_info)))\n",
    "\n",
    "# 提取有效出行数据\n",
    "valid_travel_info2 = travel_info[travel_info['position'].isin(valid_positions2)]\n",
    "valid_travel_info2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"剔除总行程时间异常后的最终出行次数: {len(valid_travel_info2)} / {len(travel_info)}\", \n",
    "      \"占比: {:.2%}\".format((len(valid_travel_info2)) / len(travel_info)))"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "929bd2d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T21:46:44.290687Z",
     "start_time": "2025-08-04T21:46:31.833084Z"
    }
   },
   "source": [
    "# todo 统计路网出行信息\n",
    "print('路网平均运行速度(km/h):', round(np.mean(valid_travel_info2['speed_kmh']), 2), \n",
    "      '中位数:', round(np.median(valid_travel_info2['speed_kmh']), 2),\n",
    "      '最小值:', round(np.min(valid_travel_info2['speed_kmh']), 2),\n",
    "      '最大值:', round(np.max(valid_travel_info2['speed_kmh']), 2))\n",
    "print('平均出行距离(km):', round(np.mean(valid_travel_info2['distance_km']), 2),\n",
    "      '中位数:', round(np.median(valid_travel_info2['distance_km']), 2),\n",
    "      '最小值:', round(np.min(valid_travel_info2['distance_km']), 2),\n",
    "      '最大值:', round(np.max(valid_travel_info2['distance_km']), 2))\n",
    "print('平均出行时间(min):', round(np.mean(valid_travel_info2['travel_time_s']) / 60, 2),\n",
    "      '中位数:', round(np.median(valid_travel_info2['travel_time_s']) / 60, 2),\n",
    "      '最小值:', round(np.min(valid_travel_info2['travel_time_s']) / 60, 2),\n",
    "      '最大值:', round(np.max(valid_travel_info2['travel_time_s']) / 60, 2))\n",
    "\n",
    "valid_travel_info2.to_csv(save_file + '/' + 'travel_info.csv', encoding='gbk', index=False)\n",
    "valid_travel_info2.head(3)"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "29de2fd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T05:36:03.925764Z",
     "start_time": "2025-08-07T05:36:03.678781Z"
    }
   },
   "source": [
    "# # 单次出行经过的路网节点及其经纬度、过车时刻、对应卡口设备\n",
    "# \n",
    "# nodes_info = [[Complete_path_pos[i] for i in valid_positions2], \n",
    "#               [Complete_path_coords[i] for i in valid_positions2], \n",
    "#               [Complete_path_time[i] for i in valid_positions2], \n",
    "#               [Complete_path_distance[i] for i in valid_positions2], \n",
    "#               [Complete_path_bayonet[i] for i in valid_positions2]]\n",
    "# # # 保存为 .pkl 文件\n",
    "# # with open(save_file + '/' + \"nodes_info.pkl\", 'wb') as f:\n",
    "# #     pickle.dump(nodes_info, f)\n",
    "# # 定义批次大小\n",
    "# batch_size = 20000  # 根据你的内存情况调整这个值\n",
    "# \n",
    "# # 获取总长度\n",
    "# total_length = len(valid_positions2)\n",
    "# print(f\"总数据量: {total_length}\")\n",
    "# \n",
    "# # 分批处理并收集所有数据\n",
    "# all_pos = []\n",
    "# all_coords = []\n",
    "# all_time = []\n",
    "# all_distance = []\n",
    "# all_bayonet = []\n",
    "# \n",
    "# # 分批处理\n",
    "# for batch_start in range(0, total_length, batch_size):\n",
    "#     batch_end = min(batch_start + batch_size, total_length)\n",
    "#     batch_indices = valid_positions2[batch_start:batch_end]\n",
    "#     \n",
    "#     # 处理当前批次\n",
    "#     batch_pos = [Complete_path_pos[i] for i in batch_indices]\n",
    "#     batch_coords = [Complete_path_coords[i] for i in batch_indices]\n",
    "#     batch_time = [Complete_path_time[i] for i in batch_indices]\n",
    "#     batch_distance = [Complete_path_distance[i] for i in batch_indices]\n",
    "#     batch_bayonet = [Complete_path_bayonet[i] for i in batch_indices]\n",
    "#     \n",
    "#     # 收集批次数据\n",
    "#     all_pos.extend(batch_pos)\n",
    "#     all_coords.extend(batch_coords)\n",
    "#     all_time.extend(batch_time)\n",
    "#     all_distance.extend(batch_distance)\n",
    "#     all_bayonet.extend(batch_bayonet)\n",
    "#     \n",
    "#     print(f\"已处理批次: {batch_start}-{batch_end}\")\n",
    "# \n",
    "# # 组装最终数据\n",
    "# nodes_info = [all_pos, all_coords, all_time, all_distance, all_bayonet]\n",
    "# \n",
    "# # 保存为单个 .pkl 文件\n",
    "# with open(save_file + '/' + \"nodes_info.pkl\", 'wb') as f:\n",
    "#     pickle.dump(nodes_info, f)\n",
    "# \n",
    "# # 打印结果\n",
    "# print('nodes_info saved to:', save_file + '/' + \"nodes_info.pkl\")\n",
    "# print('nodes_info shape:', np.array(nodes_info).shape if len(nodes_info[0]) > 0 else \"Empty data\")\n",
    "# \n",
    "# # 可选：清理中间变量释放内存\n",
    "# del all_pos, all_coords, all_time, all_distance, all_bayonet\n",
    "# \n",
    "# # print('* node:', nodes_info[0][0])\n",
    "# # print('* coord:', nodes_info[1][0])\n",
    "# # print('* time:', nodes_info[2][0])\n",
    "# # print('* distance(m):', nodes_info[3][0])\n",
    "# # print('* bayonet:', nodes_info[4][0])\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 定义批次大小\n",
    "batch_size = 5000  # 进一步减小批次大小\n",
    "\n",
    "# 获取总长度\n",
    "total_length = len(valid_positions2)\n",
    "print(f\"总数据量: {total_length}\")\n",
    "\n",
    "# 使用流式写入方式保存数据\n",
    "def stream_save_nodes_info(valid_positions2, save_file, batch_size=5000):\n",
    "    \"\"\"流式保存节点信息，避免内存溢出\"\"\"\n",
    "    \n",
    "    os.makedirs(save_file, exist_ok=True)\n",
    "    \n",
    "    # 创建临时文件来存储各个部分\n",
    "    temp_files = []\n",
    "    part_count = 0\n",
    "    \n",
    "    # 分批处理并保存每个部分\n",
    "    for batch_start in range(0, total_length, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_length)\n",
    "        batch_indices = valid_positions2[batch_start:batch_end]\n",
    "        \n",
    "        # 处理当前批次\n",
    "        batch_data = [\n",
    "            [Complete_path_pos[i] for i in batch_indices],\n",
    "            [Complete_path_coords[i] for i in batch_indices],\n",
    "            [Complete_path_time[i] for i in batch_indices],\n",
    "            [Complete_path_distance[i] for i in batch_indices],\n",
    "            [Complete_path_bayonet[i] for i in batch_indices]\n",
    "        ]\n",
    "        \n",
    "        # 保存临时批次文件\n",
    "        temp_file = os.path.join(save_file, f\"temp_part_{part_count}.pkl\")\n",
    "        with open(temp_file, 'wb') as f:\n",
    "            pickle.dump(batch_data, f)\n",
    "        \n",
    "        temp_files.append(temp_file)\n",
    "        part_count += 1\n",
    "        \n",
    "        print(f\"已保存部分: {part_count} (处理了 {batch_end}/{total_length} 条记录)\")\n",
    "    \n",
    "    print(f\"共保存了 {part_count} 个临时部分\")\n",
    "    \n",
    "    # 逐个读取临时文件并合并保存\n",
    "    print(\"开始合并并保存最终文件...\")\n",
    "    \n",
    "    final_file = os.path.join(save_file, \"nodes_info.pkl\")\n",
    "    \n",
    "    # 分别处理每个数据维度\n",
    "    with open(final_file, 'wb') as final_f:\n",
    "        # 创建最终数据结构\n",
    "        final_data = [[], [], [], [], []]\n",
    "        \n",
    "        # 逐个读取临时文件\n",
    "        for i, temp_file in enumerate(temp_files):\n",
    "            with open(temp_file, 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "            \n",
    "            # 扩展最终数据\n",
    "            for j in range(5):\n",
    "                final_data[j].extend(batch_data[j])\n",
    "            \n",
    "            # 删除临时文件释放空间\n",
    "            os.remove(temp_file)\n",
    "            print(f\"已处理临时文件 {i+1}/{len(temp_files)}\")\n",
    "        \n",
    "        # 保存最终数据\n",
    "        pickle.dump(final_data, final_f)\n",
    "    \n",
    "    print('nodes_info saved to:', final_file)\n",
    "    print('nodes_info shape:', np.array(final_data).shape if len(final_data[0]) > 0 else \"Empty data\")\n",
    "    \n",
    "    return final_file\n",
    "\n",
    "# 使用流式保存\n",
    "try:\n",
    "    saved_file = stream_save_nodes_info(valid_positions2, save_file, batch_size=5000)\n",
    "    print(\"保存成功!\")\n",
    "except Exception as e:\n",
    "    print(f\"保存失败: {e}\")\n",
    "    # 如果还是失败，使用纯分批保存方案\n",
    "    print(\"使用纯分批保存方案...\")\n",
    "    \n",
    "    def save_as_separate_batches(valid_positions2, save_file, batch_size=5000):\n",
    "        \"\"\"保存为独立的批次文件\"\"\"\n",
    "        \n",
    "        os.makedirs(save_file, exist_ok=True)\n",
    "        \n",
    "        total_length = len(valid_positions2)\n",
    "        num_batches = (total_length + batch_size - 1) // batch_size\n",
    "        \n",
    "        # 保存批次信息\n",
    "        batch_info = {\n",
    "            'total_length': total_length,\n",
    "            'batch_size': batch_size,\n",
    "            'num_batches': num_batches,\n",
    "            'data_structure': ['pos', 'coords', 'time', 'distance', 'bayonet']\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(save_file, \"batch_info.pkl\"), 'wb') as f:\n",
    "            pickle.dump(batch_info, f)\n",
    "        \n",
    "        # 保存每个批次\n",
    "        for batch_idx, batch_start in enumerate(range(0, total_length, batch_size)):\n",
    "            batch_end = min(batch_start + batch_size, total_length)\n",
    "            batch_indices = valid_positions2[batch_start:batch_end]\n",
    "            \n",
    "            batch_data = [\n",
    "                [Complete_path_pos[i] for i in batch_indices],\n",
    "                [Complete_path_coords[i] for i in batch_indices],\n",
    "                [Complete_path_time[i] for i in batch_indices],\n",
    "                [Complete_path_distance[i] for i in batch_indices],\n",
    "                [Complete_path_bayonet[i] for i in batch_indices]\n",
    "            ]\n",
    "            \n",
    "            batch_filename = f\"nodes_info_batch_{batch_idx:04d}.pkl\"\n",
    "            with open(os.path.join(save_file, batch_filename), 'wb') as f:\n",
    "                pickle.dump(batch_data, f)\n",
    "            \n",
    "            print(f\"批次 {batch_idx+1}/{num_batches} 已保存\")\n",
    "        \n",
    "        print(f\"数据已保存为 {num_batches} 个独立批次文件\")\n",
    "        print(\"使用 load_batched_data(save_file) 来加载数据\")\n",
    "    \n",
    "    save_as_separate_batches(valid_positions2, save_file, batch_size=5000)\n",
    "    \n",
    "    # 提供加载函数\n",
    "    def load_batched_data(save_path):\n",
    "        \"\"\"加载分批保存的数据\"\"\"\n",
    "        # 加载批次信息\n",
    "        with open(os.path.join(save_path, \"batch_info.pkl\"), 'rb') as f:\n",
    "            batch_info = pickle.load(f)\n",
    "        \n",
    "        # 合并所有批次\n",
    "        merged_data = [[] for _ in range(5)]\n",
    "        \n",
    "        for batch_idx in range(batch_info['num_batches']):\n",
    "            batch_filename = f\"nodes_info_batch_{batch_idx:04d}.pkl\"\n",
    "            with open(os.path.join(save_path, batch_filename), 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "            \n",
    "            for i in range(5):\n",
    "                merged_data[i].extend(batch_data[i])\n",
    "        \n",
    "        return merged_data\n",
    "    \n",
    "    print(\"分批保存完成！\")\n",
    "    print(\"加载数据请使用: nodes_info = load_batched_data(save_file)\")"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "79ea25ba",
   "metadata": {},
   "source": [
    "# 创建速度估计的输入数据"
   ]
  },
  {
   "cell_type": "code",
   "id": "c07071fc",
   "metadata": {},
   "source": [
    "# 新建存储列表\n",
    "device_id_array = []\n",
    "datetime_array = []\n",
    "bayonet_array = []\n",
    "node_array = []\n",
    "longitude_array = []\n",
    "latitude_array = []\n",
    "travel_id_array = []\n",
    "c_distance_array = []\n",
    "distance_array = []\n",
    "\n",
    "# 遍历每一次补全信息成功的单次出行路径\n",
    "for i in range(0, len(valid_positions2)):\n",
    "    \n",
    "    # 补全信息成功的路径索引\n",
    "    index = valid_positions2[i]\n",
    "    travel_id = valid_positions2[i]\n",
    "    device_id = path_cltmbh[index]\n",
    "    \n",
    "    # 遍历经过的每一路网节点（卡口）\n",
    "    c_distance = 0\n",
    "    for j in range(0, len(Complete_path_pos[index])):\n",
    "        datetime = Complete_path_time[index][j]\n",
    "        node = Complete_path_pos[index][j]\n",
    "        bayonet = Complete_path_bayonet[index][j]\n",
    "        longitude = Complete_path_coords[index][j][0]\n",
    "        latitude = Complete_path_coords[index][j][1]\n",
    "        distance = Complete_path_distance[index][j]\n",
    "        c_distance += distance\n",
    "        # 保存结果\n",
    "        device_id_array.append(device_id)\n",
    "        datetime_array.append(datetime)\n",
    "        bayonet_array.append(bayonet)\n",
    "        node_array.append(node)\n",
    "        longitude_array.append(longitude)\n",
    "        latitude_array.append(latitude)\n",
    "        travel_id_array.append(travel_id)\n",
    "        c_distance_array.append(c_distance)\n",
    "        distance_array.append(distance)\n",
    "    \n",
    "    # 定期打印运行结果和保存数据\n",
    "    if i in list(np.arange(20000, 400001, 20000)):\n",
    "        print(f\"第{i}条路径 记录完成\")\n",
    "        \n",
    "print(1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ad49cbb8",
   "metadata": {},
   "source": [
    "# 构建dataframe\n",
    "input_df = {'device_id': device_id_array, \n",
    "            'travel_id': travel_id_array, \n",
    "            'datetime': datetime_array, \n",
    "            'bayonetname': bayonet_array, \n",
    "            'posname': node_array, \n",
    "            'longitude': longitude_array, \n",
    "            'latitude': latitude_array, \n",
    "            'c_distance': c_distance_array, \n",
    "            'distance': distance_array}\n",
    "input_df = pd.DataFrame(input_df)\n",
    "\n",
    "# 保存输入数据\n",
    "input_df.to_csv(save_file + '/' + 'path_result.csv', index=False, encoding='gbk')\n",
    "\n",
    "print('速度估计的输入数据量:', len(input_df))\n",
    "input_df.head(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "239fa40d",
   "metadata": {},
   "source": [
    "input_df.info(verbose=False, memory_usage=\"deep\")  # 查看文件大小和占用运行内存"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "330b60e0",
   "metadata": {},
   "source": [
    "# 保存输入数据\n",
    "input_df.to_csv(save_file + '/' + 'path_result.csv', index=False, encoding='gbk')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "68b0257f",
   "metadata": {},
   "source": [
    "# 统计路径重构后相邻路网节点之间的时间间隔"
   ]
  },
  {
   "cell_type": "code",
   "id": "1603c482",
   "metadata": {},
   "source": [
    "# 计算时间间隔\n",
    "time_interval = []\n",
    "for i in range(0, len(Complete_path_time)):\n",
    "    if Complete_path_tf[i] == \"get\":\n",
    "        pathtimes = Complete_path_time[i]\n",
    "        for j in range(1, len(pathtimes)):\n",
    "            diff = (pathtimes[j] - pathtimes[j-1]).seconds\n",
    "            time_interval.append(diff)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "print('最小间隔(s):', min(time_interval))\n",
    "print('最大间隔(s):', max(time_interval))\n",
    "print('平均间隔(s):',  round(np.mean(time_interval)))\n",
    "print('中位数间隔(s):',  round(np.median(time_interval)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "675fa716",
   "metadata": {},
   "source": [
    "# 统计时间间隔\n",
    "interval = [0, 5, 15, 30, 60, 300, 900, 1800, 3600, 28800, 43200, 86400]   # 时间间隔\n",
    "label = [5, 15, 30, 60, 300, 900, 1800, 3600, 28800, 43200, 86400]\n",
    "a = pd.cut(time_interval, interval, labels=label)   # 区间左开右闭\n",
    "counts = pd.value_counts(a)\n",
    "counts = pd.DataFrame(counts)\n",
    "counts.reset_index(drop=False, inplace=True)\n",
    "counts.columns = ['travel_time_s',  'count']\n",
    "counts.sort_values(by='travel_time_s', ascending=True, inplace=True)  # 按照从小到大排序\n",
    "counts.reset_index(drop=True, inplace=True)\n",
    "counts['perc'] = counts['count'] / sum(counts['count'])\n",
    "counts.to_excel('相邻路网节点的时间间隔分布.xlsx', index=False)  # 保存数据\n",
    "counts"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8363fecd",
   "metadata": {},
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
